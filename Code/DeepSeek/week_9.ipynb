{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "7mE3rPtV7oge",
        "outputId": "08f49fb0-2efd-4434-a2d1-1c5e1114c1c9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3bda85d5-95b0-4424-a179-801a1a5560dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3bda85d5-95b0-4424-a179-801a1a5560dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PdM_forecasting.xlsx to PdM_forecasting.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pdm datasets**"
      ],
      "metadata": {
        "id": "q_r8wPu3pi0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from openai import OpenAI\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "client = OpenAI(api_key=\"sk-1545d86e319f4ba7bf46df9f0c69d90b\", base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "def get_prediction_length(sheet_name):\n",
        "    match = re.search(r'_(\\d+)$', sheet_name)\n",
        "    if match:\n",
        "        data_length = int(match.group(1))\n",
        "        if data_length < 180:\n",
        "            return 24\n",
        "        elif data_length < 300:\n",
        "            return 36\n",
        "        elif data_length < 1000:\n",
        "            return 48\n",
        "        else:\n",
        "            return 100\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid sheet name: {sheet_name}\")\n",
        "\n",
        "def extract_float_list(text):\n",
        "    match = re.search(r'\\[([0-9eE+.,\\s-]+)\\]', text)\n",
        "    if not match:\n",
        "        raise ValueError(\"No valid list found in model response.\")\n",
        "    number_str = match.group(1)\n",
        "    return [float(n) for n in number_str.strip().split(\",\") if n.strip() != \"\"]\n",
        "\n",
        "def summarize_metrics(df):\n",
        "    df[\"MAPE\"] = df[\"MAPE\"].abs()\n",
        "    df[\"MAE\"] = df[\"MAE\"].abs()\n",
        "    return df[[\"MAPE\", \"MAE\"]].describe().T\n",
        "\n",
        "excel_file = \"/content/PdM_forecasting.xlsx\"\n",
        "sheet_names = pd.ExcelFile(excel_file).sheet_names\n",
        "\n",
        "results = []\n",
        "\n",
        "for sheet_name in sheet_names:\n",
        "    try:\n",
        "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "        col = df.select_dtypes(include=['number']).columns[0]\n",
        "        series = df[col].dropna().tolist()\n",
        "\n",
        "        pred_len = get_prediction_length(sheet_name)\n",
        "        if len(series) < pred_len + 5:\n",
        "            print(f\"❌ Skipping {sheet_name}: insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        input_series = series[:-pred_len]\n",
        "        target_series = series[-pred_len:]\n",
        "\n",
        "        prompt = (\n",
        "            f\"I have a time series with {len(input_series)} values.\\n\"\n",
        "            f\"Series: {input_series}\\n\"\n",
        "            f\"Please forecast the next {pred_len} values in the same format \"\n",
        "            f\"(as a Python list, just the numbers).\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a forecasting expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predicted_str = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            predicted = extract_float_list(predicted_str)\n",
        "            if len(predicted) != pred_len:\n",
        "                raise ValueError(\"Prediction length mismatch.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Prediction parse failed for {sheet_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mae = mean_absolute_error(target_series, predicted)\n",
        "        mape = mean_absolute_percentage_error(target_series, predicted)\n",
        "\n",
        "        results.append({\n",
        "            \"Sheet\": sheet_name,\n",
        "            \"MAE\": mae,\n",
        "            \"MAPE\": mape,\n",
        "        })\n",
        "\n",
        "        print(f\"{sheet_name} → MAE: {mae:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in {sheet_name}: {e}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/deepseek_forecasting_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n📊 Summary of Forecasting Performance:\")\n",
        "print(summarize_metrics(results_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jFlDZdh896g",
        "outputId": "80129c9b-98bd-4b9b-da24-f1a8d4e3d228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "volt_200 → MAE: 16.2673, MAPE: 0.0958\n",
            "volt_500 → MAE: 12.8748, MAPE: 0.0819\n",
            "❌ Prediction parse failed for volt_2000: No valid list found in model response.\n",
            "volt_5000 → MAE: 161.2092, MAPE: 0.9503\n",
            "❌ Prediction parse failed for volt_8761: Prediction length mismatch.\n",
            "rotate_200 → MAE: 32.6420, MAPE: 0.0729\n",
            "rotate_500 → MAE: 51.6937, MAPE: 0.1142\n",
            "❌ Prediction parse failed for rotate_2000: Prediction length mismatch.\n",
            "❌ Prediction parse failed for rotate_5000: Prediction length mismatch.\n",
            "rotate_8761 → MAE: 49.4138, MAPE: 0.1079\n",
            "pressure_200 → MAE: 7.9624, MAPE: 0.0791\n",
            "❌ Prediction parse failed for pressure_500: Prediction length mismatch.\n",
            "❌ Prediction parse failed for pressure_2000: Prediction length mismatch.\n",
            "❌ Prediction parse failed for pressure_5000: Prediction length mismatch.\n",
            "❌ Prediction parse failed for pressure_8761: Prediction length mismatch.\n",
            "vibration_200 → MAE: 5.5090, MAPE: 0.1479\n",
            "❌ Prediction parse failed for vibration_500: Prediction length mismatch.\n",
            "vibration_2000 → MAE: 5.3829, MAPE: 0.1436\n",
            "❌ Prediction parse failed for vibration_5000: Prediction length mismatch.\n",
            "❌ Prediction parse failed for vibration_8761: Prediction length mismatch.\n",
            "\n",
            "📊 Summary of Forecasting Performance:\n",
            "      count       mean        std       min       25%        50%        75%  \\\n",
            "MAPE    9.0   0.199284   0.282888  0.072851  0.081915   0.107926   0.143625   \n",
            "MAE     9.0  38.106129  49.563464  5.382870  7.962363  16.267322  49.413815   \n",
            "\n",
            "             max  \n",
            "MAPE    0.950259  \n",
            "MAE   161.209221  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "failed_sheets = [\n",
        "    \"volt_2000\", \"volt_8761\",\n",
        "    \"rotate_2000\", \"rotate_5000\",\n",
        "    \"pressure_500\", \"pressure_2000\", \"pressure_5000\", \"pressure_8761\",\n",
        "    \"vibration_500\", \"vibration_5000\", \"vibration_8761\"\n",
        "]\n",
        "\n",
        "retry_results = []\n",
        "\n",
        "for sheet_name in failed_sheets:\n",
        "    try:\n",
        "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "        col = df.select_dtypes(include=['number']).columns[0]\n",
        "        series = df[col].dropna().tolist()\n",
        "\n",
        "        pred_len = get_prediction_length(sheet_name)\n",
        "        if len(series) < pred_len + 5:\n",
        "            print(f\"❌ Skipping {sheet_name}: insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        input_series = series[:-pred_len]\n",
        "        target_series = series[-pred_len:]\n",
        "\n",
        "        prompt = (\n",
        "            f\"I have a time series with {len(input_series)} values.\\n\"\n",
        "            f\"Series: {input_series}\\n\"\n",
        "            f\"Please forecast the next {pred_len} values in the same format \"\n",
        "            f\"(as a Python list, just the numbers).\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a forecasting expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predicted_str = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            predicted = extract_float_list(predicted_str)\n",
        "\n",
        "            if len(predicted) < pred_len:\n",
        "                raise ValueError(f\"Too short prediction: expected {pred_len}, got {len(predicted)}\")\n",
        "            elif len(predicted) > pred_len:\n",
        "                print(f\"⚠️ Truncating prediction for {sheet_name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                predicted = predicted[:pred_len]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Retry parse failed for {sheet_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mae = mean_absolute_error(target_series, predicted)\n",
        "        mape = mean_absolute_percentage_error(target_series, predicted)\n",
        "\n",
        "        retry_results.append({\n",
        "            \"Sheet\": sheet_name,\n",
        "            \"MAE\": mae,\n",
        "            \"MAPE\": mape,\n",
        "        })\n",
        "\n",
        "        print(f\"[RETRY] {sheet_name} → MAE: {mae:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Retry error in {sheet_name}: {e}\")\n",
        "\n",
        "retry_df = pd.DataFrame(retry_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsYfJy4YLha4",
        "outputId": "dd9c4c28-19c8-4e1c-fe07-1c74b341e45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Truncating prediction for volt_2000: expected 100, got 114\n",
            "[RETRY] volt_2000 → MAE: 59.6492, MAPE: 0.3580\n",
            "[RETRY] volt_8761 → MAE: 12.5964, MAPE: 0.0778\n",
            "[RETRY] rotate_2000 → MAE: 44.2246, MAPE: 0.0982\n",
            "[RETRY] rotate_5000 → MAE: 149.3804, MAPE: 0.3319\n",
            "[RETRY] pressure_500 → MAE: 8.6206, MAPE: 0.0917\n",
            "⚠️ Truncating prediction for pressure_2000: expected 100, got 128\n",
            "[RETRY] pressure_2000 → MAE: 61.4354, MAPE: 0.6045\n",
            "[RETRY] pressure_5000 → MAE: 7.7923, MAPE: 0.0784\n",
            "❌ Retry parse failed for pressure_8761: Too short prediction: expected 100, got 90\n",
            "[RETRY] vibration_500 → MAE: 5.5560, MAPE: 0.1529\n",
            "❌ Retry parse failed for vibration_5000: Too short prediction: expected 100, got 98\n",
            "[RETRY] vibration_8761 → MAE: 5.0189, MAPE: 0.1368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_failed = [\"pressure_8761\", \"vibration_5000\"]\n",
        "\n",
        "retry2_results = []\n",
        "\n",
        "for sheet_name in last_failed:\n",
        "    try:\n",
        "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
        "        col = df.select_dtypes(include=['number']).columns[0]\n",
        "        series = df[col].dropna().tolist()\n",
        "\n",
        "        pred_len = get_prediction_length(sheet_name)\n",
        "        if len(series) < pred_len + 5:\n",
        "            print(f\"⚠️ {sheet_name} has very limited data: only {len(series)} total points\")\n",
        "\n",
        "        input_series = series[:-pred_len]\n",
        "        target_series = series[-pred_len:]\n",
        "\n",
        "        prompt = (\n",
        "            f\"I have a time series with {len(input_series)} values.\\n\"\n",
        "            f\"Series: {input_series}\\n\"\n",
        "            f\"Please forecast the next {pred_len} values in the same format \"\n",
        "            f\"(as a Python list, just the numbers).\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a forecasting expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predicted_str = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            predicted = extract_float_list(predicted_str)\n",
        "\n",
        "            if len(predicted) < pred_len * 0.9:\n",
        "                raise ValueError(f\"Too short prediction: expected {pred_len}, got {len(predicted)}\")\n",
        "            elif len(predicted) < pred_len:\n",
        "                print(f\"⚠️ Accepting shorter prediction for {sheet_name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                target_series = target_series[:len(predicted)]\n",
        "            elif len(predicted) > pred_len:\n",
        "                print(f\"⚠️ Truncating prediction for {sheet_name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                predicted = predicted[:pred_len]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Final parse failed for {sheet_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mae = mean_absolute_error(target_series, predicted)\n",
        "        mape = mean_absolute_percentage_error(target_series, predicted)\n",
        "\n",
        "        retry2_results.append({\n",
        "            \"Sheet\": sheet_name,\n",
        "            \"MAE\": mae,\n",
        "            \"MAPE\": mape,\n",
        "        })\n",
        "\n",
        "        print(f\"[FINAL RUN] {sheet_name} → MAE: {mae:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Final error in {sheet_name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7W-0UqQdCA1",
        "outputId": "ed4005db-f2c0-4d69-fbca-21bfee8319cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Truncating prediction for pressure_8761: expected 100, got 105\n",
            "[FINAL RUN] pressure_8761 → MAE: 11.1635, MAPE: 0.1196\n",
            "[FINAL RUN] vibration_5000 → MAE: 5.5541, MAPE: 0.1434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pdm results**"
      ],
      "metadata": {
        "id": "oFcplEj2pqwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retry2_df = pd.DataFrame(retry2_results)\n",
        "\n",
        "all_results_df = pd.concat([results_df, retry_df, retry2_df], ignore_index=True)\n",
        "all_results_df = all_results_df.drop_duplicates(subset=\"Sheet\", keep=\"last\").reset_index(drop=True)\n",
        "\n",
        "print(\"\\n✅ Final Combined Forecasting Performance:\")\n",
        "print(summarize_metrics(all_results_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02F9jZUYej10",
        "outputId": "a41c149b-15a1-4aee-8046-94ab76a844d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Final Combined Forecasting Performance:\n",
            "      count       mean        std       min       25%        50%        75%  \\\n",
            "MAPE   20.0   0.199342   0.219321  0.072851  0.089239   0.116861   0.149154   \n",
            "MAE    20.0  35.697327  45.504712  5.018946  7.233217  12.735615  49.983788   \n",
            "\n",
            "             max  \n",
            "MAPE    0.950259  \n",
            "MAE   161.209221  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/datasets\")"
      ],
      "metadata": {
        "id": "rmHgEn-EhyQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"sk-1545d86e319f4ba7bf46df9f0c69d90b\", base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "# Step 1: Determine prediction length from filename\n",
        "def get_prediction_length(file_name):\n",
        "    match = re.search(r'_(\\d+)', file_name)\n",
        "    if match:\n",
        "        data_length = int(match.group(1))\n",
        "        if data_length < 180:\n",
        "            return 24\n",
        "        elif data_length < 300:\n",
        "            return 36\n",
        "        elif data_length < 1000:\n",
        "            return 48\n",
        "        else:\n",
        "            return 100\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid file name: {file_name}\")\n",
        "\n",
        "# Step 2: Extract float list from DeepSeek response\n",
        "def extract_float_list(text):\n",
        "    match = re.search(r'\\[([0-9eE+.,\\s-]+)\\]', text)\n",
        "    if not match:\n",
        "        raise ValueError(\"No valid list found in model response.\")\n",
        "    number_str = match.group(1)\n",
        "    return [float(n) for n in number_str.strip().split(\",\") if n.strip() != \"\"]\n",
        "\n",
        "# Step 3: Summarize evaluation metrics\n",
        "def summarize_metrics(df):\n",
        "    df[\"MAPE\"] = df[\"MAPE\"].abs()\n",
        "    df[\"MAE\"] = df[\"MAE\"].abs()\n",
        "    return df[[\"MAPE\", \"MAE\"]].describe().T\n",
        "\n",
        "results = []\n",
        "\n",
        "for file_path in csv_files:\n",
        "    try:\n",
        "        file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
        "        pred_len = get_prediction_length(file_name)\n",
        "\n",
        "        df = pd.read_csv(file_path)\n",
        "        col = df.select_dtypes(include=['number']).columns[0]\n",
        "        series = df[col].dropna().tolist()\n",
        "\n",
        "        if len(series) < pred_len + 5:\n",
        "            print(f\"❌ Skipping {file_name}: insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        input_series = series[:-pred_len]\n",
        "        target_series = series[-pred_len:]\n",
        "\n",
        "        prompt = (\n",
        "            f\"I have a time series with {len(input_series)} values.\\n\"\n",
        "            f\"Series: {input_series}\\n\"\n",
        "            f\"Please forecast the next {pred_len} values in the same format \"\n",
        "            f\"(as a Python list, just the numbers).\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a forecasting expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predicted_str = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            predicted = extract_float_list(predicted_str)\n",
        "\n",
        "            if len(predicted) < pred_len * 0.9:\n",
        "                raise ValueError(f\"Too short prediction: expected {pred_len}, got {len(predicted)}\")\n",
        "            elif len(predicted) < pred_len:\n",
        "                print(f\"⚠️ Accepting shorter prediction for {file_name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                target_series = target_series[:len(predicted)]\n",
        "            elif len(predicted) > pred_len:\n",
        "                print(f\"⚠️ Truncating prediction for {file_name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                predicted = predicted[:pred_len]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Prediction parse failed for {file_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mae = mean_absolute_error(target_series, predicted)\n",
        "        mape = mean_absolute_percentage_error(target_series, predicted)\n",
        "\n",
        "        results.append({\n",
        "            \"File\": file_name,\n",
        "            \"MAE\": mae,\n",
        "            \"MAPE\": mape,\n",
        "        })\n",
        "\n",
        "        print(f\"{file_name} → MAE: {mae:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in {file_name}: {e}\")\n",
        "\n",
        "# Final results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n📊 Summary of Forecasting Performance:\")\n",
        "print(summarize_metrics(results_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M_rw0iLi9tC",
        "outputId": "758c92ac-2792-4885-d7b6-6128bba7c5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "共找到 80 个 CSV 文件\n",
            "❌ Error in ETTh2Dataset_DARTS_OT_10000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 69169 tokens (69169 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "SunspotsDataset_DARTS_2820 → MAE: 71.0920, MAPE: 2.0054\n",
            "ExchangeRateDataset_DARTS_chn_1000 → MAE: 0.0000, MAPE: 0.0000\n",
            "ILINetDataset_DARTS_age25to64_364 → MAE: 2391.6875, MAPE: 0.7337\n",
            "TemperatureDataset_DARTS_200 → MAE: 3.4528, MAPE: 0.5798\n",
            "USGasolineDataset_DARTS_1578 → MAE: 2698.6200, MAPE: 0.3424\n",
            "⚠️ Truncating prediction for ETTm1Dataset_DARTS_OT_1000: expected 100, got 114\n",
            "ETTm1Dataset_DARTS_OT_1000 → MAE: 3.5349, MAPE: 0.1191\n",
            "❌ Error in ETTm1Dataset_DARTS_OT_10000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 69135 tokens (69135 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "HeaterDataset_DARTS_198 → MAE: 6.6667, MAPE: 0.1870\n",
            "ExchangeRateDataset_DARTS_aus_7588 → MAE: 0.0000, MAPE: 0.0000\n",
            "⚠️ Truncating prediction for ILINetDataset_DARTS_age50to64_677: expected 48, got 90\n",
            "ILINetDataset_DARTS_age50to64_677 → MAE: 1793.4583, MAPE: 0.3833\n",
            "❌ Error in ETTm2Dataset_DARTS_OT_10000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 69192 tokens (69192 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "ExchangeRateDataset_DARTS_brt_1000 → MAE: 0.0000, MAPE: 0.0000\n",
            "❌ Error in ETTh1Dataset_DARTS_OT_17420: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 153161 tokens (153161 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "❌ Prediction parse failed for AirQuality_UCI_ML_Repo_O3_9357: No valid list found in model response.\n",
            "⚠️ Truncating prediction for ETTm2Dataset_DARTS_OT_1000: expected 100, got 110\n",
            "ETTm2Dataset_DARTS_OT_1000 → MAE: 2.0457, MAPE: 0.0513\n",
            "ExchangeRateDataset_DARTS_can_7588 → MAE: 0.0000, MAPE: 0.0000\n",
            "ETTh1Dataset_DARTS_OT_3000 → MAE: 2.9259, MAPE: 0.2445\n",
            "⚠️ Truncating prediction for ETTm2Dataset_DARTS_OT_3000: expected 100, got 110\n",
            "ETTm2Dataset_DARTS_OT_3000 → MAE: 6.3902, MAPE: 0.1388\n",
            "ILINetDataset_DARTS_illtotal_1041 → MAE: 14686.4300, MAPE: 0.4095\n",
            "⚠️ Truncating prediction for ILINetDataset_DARTS_age0to4_1041: expected 100, got 111\n",
            "ILINetDataset_DARTS_age0to4_1041 → MAE: 5129.3800, MAPE: 0.4649\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_CO_3000: expected 100, got 119\n",
            "AirQuality_UCI_ML_Repo_CO_3000 → MAE: 237.7600, MAPE: 0.2444\n",
            "IceCreamDataset_DARTS_198 → MAE: 15.3889, MAPE: 0.3037\n",
            "ILINetDataset_DARTS_age25to49_677 → MAE: 5469.9167, MAPE: 0.5339\n",
            "AirQuality_UCI_ML_Repo_CO_9357 → MAE: 176.6800, MAPE: 0.1920\n",
            "ExchangeRateDataset_DARTS_can_1000 → MAE: 0.0000, MAPE: 0.0000\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_fossilgas_3000: expected 100, got 104\n",
            "EnergyDataset_DARTS_fossilgas_3000 → MAE: 772.9900, MAPE: 0.1758\n",
            "ExchangeRateDataset_DARTS_chn_3000 → MAE: 0.0000, MAPE: 0.0000\n",
            "CO2Dataset_DARTS_296 → MAE: 3.1361, MAPE: 0.0584\n",
            "ETTm1Dataset_DARTS_OT_3000 → MAE: 0.7978, MAPE: 0.0231\n",
            "AusBeerDataset_DARTS_211 → MAE: 42.0833, MAPE: 0.0984\n",
            "EnergyDataset_DARTS_fossialoil_10000 → MAE: 126.2480, MAPE: 0.3684\n",
            "ExchangeRateDataset_DARTS_chn_7588 → MAE: 0.0000, MAPE: 0.0000\n",
            "GasRateDataset_DARTS_296 → MAE: 0.6198, MAPE: 7631099368604.3965\n",
            "⚠️ Truncating prediction for ILINetDataset_DARTS_age65_1041: expected 100, got 109\n",
            "ILINetDataset_DARTS_age65_1041 → MAE: 2312.7800, MAPE: 1.0361\n",
            "ExchangeRateDataset_DARTS_can_3000 → MAE: 0.0000, MAPE: 0.0000\n",
            "ETTh1Dataset_DARTS_OT_1000 → MAE: 2.0576, MAPE: 0.0645\n",
            "WeatherDataset_DARTS_windspeed_1000 → MAE: 0.7172, MAPE: 0.7037\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_fossialoil_3000: expected 100, got 102\n",
            "EnergyDataset_DARTS_fossialoil_3000 → MAE: 40.3870, MAPE: 0.1315\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_solar_1000: expected 100, got 105\n",
            "EnergyDataset_DARTS_solar_1000 → MAE: 5396.9900, MAPE: 80.9102\n",
            "HeartRateDataset_DARTS_1800 → MAE: 11.4730, MAPE: 0.1371\n",
            "⚠️ Truncating prediction for ETTh2Dataset_DARTS_OT_1000: expected 100, got 105\n",
            "ETTh2Dataset_DARTS_OT_1000 → MAE: 8.7272, MAPE: 0.1928\n",
            "ExchangeRateDataset_DARTS_brt_7588 → MAE: 0.0000, MAPE: 0.0000\n",
            "⚠️ Truncating prediction for TemperatureDataset_DARTS_3650: expected 100, got 108\n",
            "TemperatureDataset_DARTS_3650 → MAE: 5.3250, MAPE: 0.4465\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_fossilgas_1000: expected 100, got 109\n",
            "EnergyDataset_DARTS_fossilgas_1000 → MAE: 3183.3360, MAPE: 0.7540\n",
            "WeatherDataset_DARTS_pressure_10000 → MAE: 3.9819, MAPE: 0.0040\n",
            "❌ Error in ETTm2Dataset_DARTS_OT_20000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 139022 tokens (139022 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "❌ Error in ETTm1Dataset_DARTS_OT_20000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 138901 tokens (138901 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "WeatherDataset_DARTS_windspeed_10000 → MAE: 1.4371, MAPE: 0.4338\n",
            "WeatherDataset_DARTS_windspeed_3000 → MAE: 0.3510, MAPE: 0.4959\n",
            "ILINetDataset_DARTS__WEIGHTED_ILI_1041 → MAE: 0.9713, MAPE: 0.5620\n",
            "⚠️ Truncating prediction for TaylorDataset_DARTS_4032: expected 100, got 115\n",
            "TaylorDataset_DARTS_4032 → MAE: 17859.4800, MAPE: 0.7124\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_solar_3000: expected 100, got 110\n",
            "EnergyDataset_DARTS_solar_3000 → MAE: 2496.1000, MAPE: 16.4019\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_fossilgas_10000: expected 100, got 110\n",
            "EnergyDataset_DARTS_fossilgas_10000 → MAE: 1873.3600, MAPE: 0.5274\n",
            "❌ Error in ETTh2Dataset_DARTS_OT_17420: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 120865 tokens (120865 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "⚠️ Truncating prediction for WeatherDataset_DARTS_pressure_3000: expected 100, got 109\n",
            "WeatherDataset_DARTS_pressure_3000 → MAE: 0.5240, MAPE: 0.0005\n",
            "⚠️ Truncating prediction for TemperatureDataset_DARTS_2000: expected 100, got 117\n",
            "TemperatureDataset_DARTS_2000 → MAE: 3.8550, MAPE: 0.4336\n",
            "❌ Error in ETTh1Dataset_DARTS_OT_10000: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 69127 tokens (69127 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
            "TemperatureDataset_DARTS_500 → MAE: 5.7979, MAPE: 0.6899\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_RH_9357: expected 100, got 117\n",
            "AirQuality_UCI_ML_Repo_RH_9357 → MAE: 24.0670, MAPE: 0.9051\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_O3_1000: expected 100, got 104\n",
            "AirQuality_UCI_ML_Repo_O3_1000 → MAE: 391.2500, MAPE: 0.5450\n",
            "AirQuality_UCI_ML_Repo_RH_1000 → MAE: 12.4290, MAPE: 0.2511\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_O3_3000: expected 100, got 106\n",
            "AirQuality_UCI_ML_Repo_O3_3000 → MAE: 1275.3300, MAPE: 1.8341\n",
            "WineDataset_DARTS_176 → MAE: 1750.1250, MAPE: 0.0713\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_fossialoil_1000: expected 100, got 103\n",
            "EnergyDataset_DARTS_fossialoil_1000 → MAE: 144.3400, MAPE: 0.5002\n",
            "⚠️ Truncating prediction for ETTh2Dataset_DARTS_OT_3000: expected 100, got 105\n",
            "ETTh2Dataset_DARTS_OT_3000 → MAE: 2.9004, MAPE: 0.1278\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_CO_1000: expected 100, got 105\n",
            "AirQuality_UCI_ML_Repo_CO_1000 → MAE: 277.7200, MAPE: 0.2804\n",
            "⚠️ Truncating prediction for WeatherDataset_DARTS_pressure_1000: expected 100, got 105\n",
            "WeatherDataset_DARTS_pressure_1000 → MAE: 0.9211, MAPE: 0.0009\n",
            "ExchangeRateDataset_DARTS_brt_3000 → MAE: 0.0000, MAPE: 0.0000\n",
            "⚠️ Truncating prediction for AirQuality_UCI_ML_Repo_RH_3000: expected 100, got 106\n",
            "AirQuality_UCI_ML_Repo_RH_3000 → MAE: 16.8220, MAPE: 0.5923\n",
            "ExchangeRateDataset_DARTS_aus_1000 → MAE: 0.0000, MAPE: 0.0000\n",
            "❌ Prediction parse failed for EnergyDataset_DARTS_solar_10000: No valid list found in model response.\n",
            "ILINetDataset_DARTS_num_provider_1041 → MAE: 707.9000, MAPE: 0.2075\n",
            "ExchangeRateDataset_DARTS_aus_3000 → MAE: 0.0000, MAPE: 0.0000\n",
            "AirPassengersDataset_DARTS_144 → MAE: 13.5833, MAPE: 0.0307\n",
            "MonthlyMilkDataset_DARTS_168 → MAE: 15.4167, MAPE: 0.0176\n",
            "⚠️ Truncating prediction for ILINetDataset_DARTS_age5to24_1041: expected 100, got 105\n",
            "ILINetDataset_DARTS_age5to24_1041 → MAE: 4030.6800, MAPE: 0.4186\n",
            "WoolyDataset_DARTS_119 → MAE: 2020.7500, MAPE: 0.4353\n",
            "SunspotsDataset_DARTS_500 → MAE: 54.3083, MAPE: 0.4631\n",
            "ILINetDataset_DARTS_total_patient_1041 → MAE: 498725.8900, MAPE: 0.2621\n",
            "\n",
            "📊 Summary of Forecasting Performance:\n",
            "      count          mean           std  min       25%        50%         75%  \\\n",
            "MAPE   70.0  1.090157e+11  9.120908e+11  0.0  0.035826   0.247813    0.520628   \n",
            "MAE    70.0  8.233048e+03  5.954789e+04  0.0  0.933654  11.950978  756.717500   \n",
            "\n",
            "               max  \n",
            "MAPE  7.631099e+12  \n",
            "MAE   4.987259e+05  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"sk-1545d86e319f4ba7bf46df9f0c69d90b\", base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "def get_prediction_length(file_name):\n",
        "    match = re.search(r'_(\\d+)', file_name)\n",
        "    if match:\n",
        "        data_length = int(match.group(1))\n",
        "        if data_length < 180:\n",
        "            return 24\n",
        "        elif data_length < 300:\n",
        "            return 36\n",
        "        elif data_length < 1000:\n",
        "            return 48\n",
        "        else:\n",
        "            return 100\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid file name: {file_name}\")\n",
        "\n",
        "def extract_float_list(text):\n",
        "    match = re.search(r'\\[([0-9eE+.,\\s-]+)\\]', text)\n",
        "    if not match:\n",
        "        raise ValueError(\"No valid list found in model response.\")\n",
        "    number_str = match.group(1)\n",
        "    return [float(n) for n in number_str.strip().split(\",\") if n.strip() != \"\"]\n",
        "\n",
        "retry_files = [\n",
        "    \"ETTh2Dataset_DARTS_OT_10000\",\n",
        "    \"ETTm1Dataset_DARTS_OT_10000\",\n",
        "    \"ETTm2Dataset_DARTS_OT_10000\",\n",
        "    \"ETTh1Dataset_DARTS_OT_17420\",\n",
        "    \"ETTm2Dataset_DARTS_OT_20000\",\n",
        "    \"ETTm1Dataset_DARTS_OT_20000\",\n",
        "    \"ETTh2Dataset_DARTS_OT_17420\",\n",
        "    \"ETTh1Dataset_DARTS_OT_10000\",\n",
        "    \"EnergyDataset_DARTS_solar_10000\",\n",
        "    \"AirQuality_UCI_ML_Repo_O3_9357\"\n",
        "]\n",
        "\n",
        "csv_files = glob.glob(\"/content/datasets/*.csv\")\n",
        "\n",
        "retry_results = []\n",
        "max_input_len = 2000\n",
        "\n",
        "for name in retry_files:\n",
        "    file_path = [f for f in csv_files if name in f]\n",
        "    if not file_path:\n",
        "        print(f\"❌ File not found for {name}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path[0])\n",
        "        col = df.select_dtypes(include=['number']).columns[0]\n",
        "        series = df[col].dropna().tolist()\n",
        "\n",
        "        pred_len = get_prediction_length(name)\n",
        "\n",
        "        if len(series) < pred_len + 10:\n",
        "            print(f\"❌ Skipping {name}: not enough data.\")\n",
        "            continue\n",
        "\n",
        "        input_series = series[-(pred_len + max_input_len):-pred_len]\n",
        "        target_series = series[-pred_len:]\n",
        "\n",
        "        prompt = (\n",
        "            f\"Forecast the next {pred_len} values based on this series:\\n\"\n",
        "            f\"{input_series}\\nReturn only a list.\"\n",
        "        )\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a forecasting expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predicted_str = response.choices[0].message.content.strip()\n",
        "\n",
        "        try:\n",
        "            predicted = extract_float_list(predicted_str)\n",
        "\n",
        "            if len(predicted) < pred_len * 0.9:\n",
        "                raise ValueError(f\"Too short: expected {pred_len}, got {len(predicted)}\")\n",
        "            elif len(predicted) > pred_len:\n",
        "                print(f\"⚠️ Truncating prediction for {name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                predicted = predicted[:pred_len]\n",
        "            elif len(predicted) < pred_len:\n",
        "                print(f\"⚠️ Accepting shorter prediction for {name}: expected {pred_len}, got {len(predicted)}\")\n",
        "                target_series = target_series[:len(predicted)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Prediction parse failed for {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        mae = mean_absolute_error(target_series, predicted)\n",
        "        mape = mean_absolute_percentage_error(target_series, predicted)\n",
        "\n",
        "        retry_results.append({\n",
        "            \"Dataset\": name,\n",
        "            \"MAE\": mae,\n",
        "            \"MAPE\": mape\n",
        "        })\n",
        "\n",
        "        print(f\"[RETRY] {name} → MAE: {mae:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Retry error in {name}: {e}\")\n",
        "\n",
        "retry_df = pd.DataFrame(retry_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVBNH_EghIOE",
        "outputId": "6f0dfc9c-9ef0-4075-f2f9-f2cc3a785a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Truncating prediction for ETTh2Dataset_DARTS_OT_10000: expected 100, got 180\n",
            "[RETRY] ETTh2Dataset_DARTS_OT_10000 → MAE: 23.3843, MAPE: 0.6358\n",
            "⚠️ Truncating prediction for ETTm1Dataset_DARTS_OT_10000: expected 100, got 110\n",
            "[RETRY] ETTm1Dataset_DARTS_OT_10000 → MAE: 1.3815, MAPE: 0.0774\n",
            "⚠️ Truncating prediction for ETTm2Dataset_DARTS_OT_10000: expected 100, got 101\n",
            "[RETRY] ETTm2Dataset_DARTS_OT_10000 → MAE: 1.5431, MAPE: 0.0598\n",
            "⚠️ Truncating prediction for ETTh1Dataset_DARTS_OT_17420: expected 100, got 106\n",
            "[RETRY] ETTh1Dataset_DARTS_OT_17420 → MAE: 3.8570, MAPE: 0.4278\n",
            "⚠️ Truncating prediction for ETTm2Dataset_DARTS_OT_20000: expected 100, got 106\n",
            "[RETRY] ETTm2Dataset_DARTS_OT_20000 → MAE: 3.2072, MAPE: 0.2191\n",
            "⚠️ Truncating prediction for ETTm1Dataset_DARTS_OT_20000: expected 100, got 106\n",
            "[RETRY] ETTm1Dataset_DARTS_OT_20000 → MAE: 2.6885, MAPE: 0.4488\n",
            "⚠️ Truncating prediction for ETTh2Dataset_DARTS_OT_17420: expected 100, got 122\n",
            "[RETRY] ETTh2Dataset_DARTS_OT_17420 → MAE: 3.5970, MAPE: 0.0988\n",
            "⚠️ Truncating prediction for ETTh1Dataset_DARTS_OT_10000: expected 100, got 102\n",
            "[RETRY] ETTh1Dataset_DARTS_OT_10000 → MAE: 11.6002, MAPE: 0.8042\n",
            "⚠️ Truncating prediction for EnergyDataset_DARTS_solar_10000: expected 100, got 103\n",
            "[RETRY] EnergyDataset_DARTS_solar_10000 → MAE: 2177.0900, MAPE: 83.1215\n",
            "[RETRY] AirQuality_UCI_ML_Repo_O3_9357 → MAE: 838.7100, MAPE: 1.3598\n",
            "\n",
            "📊 Retry finished. Saved to deepseek_forecasting_retry_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**80 datasets results**"
      ],
      "metadata": {
        "id": "2Z1VAjyktvfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.rename(columns={\"File\": \"Dataset\"})\n",
        "retry_df = retry_df.rename(columns={\"dataset\": \"Dataset\"})\n",
        "\n",
        "combined_df = pd.concat([results_df, retry_df], ignore_index=True)\n",
        "combined_df = combined_df.drop_duplicates(subset=\"Dataset\", keep=\"last\").reset_index(drop=True)\n",
        "\n",
        "summary = combined_df[[\"MAE\", \"MAPE\"]].describe().T\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ2bf1iPsqgj",
        "outputId": "242a8ff1-95e5-43fb-bb92-e721a6dcc931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      count          mean           std  min       25%        50%         75%  \\\n",
            "MAE    80.0  7.242256e+03  5.571456e+04  0.0  1.423198  10.100074  724.172500   \n",
            "MAPE   80.0  9.538874e+10  8.531828e+11  0.0  0.059472   0.256606    0.536683   \n",
            "\n",
            "               max  \n",
            "MAE   4.987259e+05  \n",
            "MAPE  7.631099e+12  \n"
          ]
        }
      ]
    }
  ]
}